{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f41e43-ca67-4223-b57d-793d2f9e6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d98fda85-00bb-48e5-b229-b3d8a1f53900",
   "metadata": {},
   "outputs": [],
   "source": [
    "film_ls=['theshoe','therock','theboyfriend','keithreynolds','cmiyc_long','busstop']\n",
    "for film in film_ls:\n",
    "    folder=f'../../Analysis Data/Description'\n",
    "    df=pd.read_csv(f'{folder}/{film}_consensus_mapped_to_neuro.csv')\n",
    "    # drop multiple ending description\n",
    "    df = df.loc[df.groupby('offset')['onset'].idxmin()].reset_index(drop=True)\n",
    "    embedding_folder = '../../Cleaned Data/Description_Cleaned_Data/embedding'\n",
    "    n_stop=df.shape[0]\n",
    "    semantic_similarity = np.zeros([n_stop,n_stop])\n",
    "    for index1, row1 in df.iterrows():\n",
    "        for index2, row2 in df.iterrows():\n",
    "            c=str(int(row1['counterbalance']))\n",
    "            seg=str(int(row1['description_stop']))\n",
    "            text1=np.load(f\"{embedding_folder}/{film}/{film}_test_c{c}_seg{seg}.npy\")\n",
    "            \n",
    "            c=str(int(row2['counterbalance']))\n",
    "            seg=str(int(row2['description_stop']))\n",
    "            text2=np.load(f\"{embedding_folder}/{film}/{film}_test_c{c}_seg{seg}.npy\")\n",
    "            \n",
    "            similarity = np.inner(np.nanmean(text1,axis=0),np.nanmean(text2,axis=0))\n",
    "            semantic_similarity[index1,index2] = similarity\n",
    "    \n",
    "    # Create the result DataFrame\n",
    "    result_df = pd.DataFrame(data=semantic_similarity,columns=df['offset'].values,index=df['offset'].values)\n",
    "    result_df['offset'] = df['offset'].values\n",
    "    output_path=f'../../Analysis Data/summary_files/semantic_similarity/{film}_semantic_similarity_byStop.csv'\n",
    "    result_df.to_csv(output_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8c6b477-3847-41ad-9a82-da2cf09ca66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "film_ls=['theshoe','therock','theboyfriend','keithreynolds','cmiyc_long','busstop']\n",
    "for film in film_ls:\n",
    "    folder=f'../../Analysis Data/Description'\n",
    "    data=pd.read_csv(f'{folder}/{film}_consensus_mapped_to_neuro.csv')\n",
    "    # drop multiple ending description\n",
    "    data = data.loc[data.groupby('offset')['onset'].idxmin()].reset_index(drop=True)\n",
    "    embedding_folder = '../../Cleaned Data/Description_Cleaned_Data/embedding'\n",
    "    n_stop=df.shape[0]\n",
    "    semantic_similarity = np.zeros([n_stop,n_stop])\n",
    "    \n",
    "    # Preload all text embeddings for easier rolling average calculation\n",
    "    text_embeddings = {}\n",
    "    for index, row in data.iterrows():\n",
    "        c = str(int(row['counterbalance']))\n",
    "        seg = str(int(row['description_stop']))\n",
    "        filename = f\"{embedding_folder}/{film}/{film}_test_c{c}_seg{seg}.npy\"\n",
    "        text_embeddings[(row['counterbalance'], row['description_stop'])] = np.load(filename)\n",
    "    # Calculate rolling average embeddings (current + two after)\n",
    "    averaged_embeddings = {}\n",
    "    for index, row in data.iterrows():\n",
    "        current_key = (row['counterbalance'], row['description_stop'])\n",
    "        current_embedding = text_embeddings[current_key]\n",
    "        \n",
    "        # Get indices of next two texts (if they exist)\n",
    "        next_indices = []\n",
    "        if index + 1 < len(data):\n",
    "            next_indices.append(index + 1)\n",
    "        if index + 2 < len(data):\n",
    "            next_indices.append(index + 2)\n",
    "        \n",
    "        # Calculate average embedding\n",
    "        if len(next_indices) < 2:\n",
    "            # If no next texts, just use current\n",
    "            averaged_embeddings[index] = np.nan\n",
    "        else:\n",
    "            # If two next texts, average with both\n",
    "            next_key1 = (data.iloc[next_indices[0]]['counterbalance'], data.iloc[next_indices[0]]['description_stop'])\n",
    "            next_key2 = (data.iloc[next_indices[1]]['counterbalance'], data.iloc[next_indices[1]]['description_stop'])\n",
    "            next_embedding1 = text_embeddings[next_key1]\n",
    "            next_embedding2 = text_embeddings[next_key2]\n",
    "            averaged_embeddings[index] = np.mean([np.mean(current_embedding,axis=0), np.mean(next_embedding1,axis=0), np.mean(next_embedding2,axis=0)], axis=0)\n",
    "    # Calculate semantic similarity with averaged embeddings\n",
    "    n=data.shape[0]-2\n",
    "    semantic_similarity = np.zeros([n,n])\n",
    "    for index1 in range(n):\n",
    "        for index2 in range(n):\n",
    "            text1 = averaged_embeddings[index1]\n",
    "            text2 = averaged_embeddings[index2]\n",
    "            similarity = np.nanmean(np.inner(text1, text2))\n",
    "            semantic_similarity[index1, index2] = similarity\n",
    "    #semantic_similarity=semantic_similarity[~np.isnan(semantic_similarity)]\n",
    "    # Create and save dataframe with results\n",
    "    df = pd.DataFrame(\n",
    "        data=semantic_similarity,\n",
    "        columns=data['offset'].values[:-2],\n",
    "        index=data['offset'].values[:-2]\n",
    "    )\n",
    "    df['TR_offset'] = df.index\n",
    "    df['offset'] = data['offset'].values[:-2]\n",
    "    output_path=f'../../Analysis Data/summary_files/semantic_similarity/{film}_rolling_semantic_similarity_byStop.csv'\n",
    "    df.to_csv(output_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d118e6d-dff0-4f4c-a8bd-ba68e1629651",
   "metadata": {},
   "source": [
    "## correlation between 2 semantic similarity measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7312ccb7-46d3-48fa-812b-0a0fe47ec637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theshoe average correlation : 0.8638069076759247\n",
      "therock average correlation : 0.921396103047374\n",
      "theboyfriend average correlation : 0.9312011819944913\n",
      "keithreynolds average correlation : 0.939579333020536\n",
      "cmiyc_long average correlation : 0.9557777764387515\n",
      "busstop average correlation : 0.9437824641005721\n"
     ]
    }
   ],
   "source": [
    "film_ls=['theshoe','therock','theboyfriend','keithreynolds','cmiyc_long','busstop']\n",
    "#film_ls=['theshoe']\n",
    "for film in film_ls:\n",
    "    filename=f'../../Analysis Data/summary_files/semantic_similarity/{film}_semantic_similarity_byStop.csv'\n",
    "    df_semantic = pd.read_csv(filename)\n",
    "    df_semantic = df_semantic.loc[:, df_semantic.columns != 'offset']\n",
    "    df_semantic=df_semantic.iloc[:,:-2]\n",
    "    filename=f'../../Analysis Data/summary_files/semantic_similarity/{film}_rolling_semantic_similarity_byStop.csv'\n",
    "    df_rolling = pd.read_csv(filename)\n",
    "    df_rolling = df_rolling[[col for col in df_rolling.columns if col not in ['offset', 'TR_offset']]]\n",
    "    r_ls=[]\n",
    "    for i in range(df_rolling.shape[0]):\n",
    "        r=np.corrcoef(df_semantic.iloc[i],df_rolling.iloc[i])[0,1]\n",
    "        r_ls.append(r)\n",
    "    print(film, \"average correlation :\",np.nanmean(r_ls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
