{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a695eb-9377-416c-ae29-e6d8c87dd310",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23936de-4e00-4040-9352-ffb01bb1d4bb",
   "metadata": {},
   "source": [
    "### Import Pacakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef9a897-6e45-4840-9d23-fcf575a325c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1301af5-a328-4825-b4e1-177357ae1672",
   "metadata": {},
   "source": [
    "### Create Unique ID for Participants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa11aa-e683-4dd8-8bc5-e23c7ad2652b",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "Extract participant system-assigned IDs from trialdata.csv files across multiple movies, and assign a new standardized user ID (e.g., user_001, user_002, etc.) to each unique participant for use in downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b0d55b-800c-44fa-9a3a-0a145884992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Reading data for: busstop / Old_2019\n",
      "  Participants in trial_data: 210\n",
      "\n",
      "ðŸ“ Reading data for: busstop / New_2021\n",
      "  Participants in trial_data: 85\n",
      "\n",
      "ðŸ“ Reading data for: cmiyc_long / Old_2019\n",
      "  Participants in trial_data: 260\n",
      "\n",
      "ðŸ“ Reading data for: cmiyc_long / New_2021\n",
      "  Participants in trial_data: 62\n",
      "\n",
      "ðŸ“ Reading data for: keithreynolds / Old_2019\n",
      "  Participants in trial_data: 198\n",
      "\n",
      "ðŸ“ Reading data for: keithreynolds / New_2021\n",
      "  Participants in trial_data: 64\n",
      "\n",
      "ðŸ“ Reading data for: theboyfriend / Old_2019\n",
      "  Participants in trial_data: 236\n",
      "\n",
      "ðŸ“ Reading data for: theboyfriend / New_2021\n",
      "  Participants in trial_data: 35\n",
      "\n",
      "ðŸ“ Reading data for: therock / Old_2019\n",
      "  Participants in trial_data: 204\n",
      "\n",
      "ðŸ“ Reading data for: therock / New_2021\n",
      "  Participants in trial_data: 77\n",
      "\n",
      "ðŸ“ Reading data for: theshoe / Old_2019\n",
      "  Participants in trial_data: 262\n",
      "\n",
      "ðŸ“ Reading data for: theshoe / New_2021\n",
      "  Participants in trial_data: 31\n"
     ]
    }
   ],
   "source": [
    "movie_list = [\"busstop\", \"cmiyc_long\", \"keithreynolds\", \"theboyfriend\", \"therock\", \"theshoe\"]\n",
    "base_path = \"../../Raw Data/Prediction Raw Psiturk Data Files\"\n",
    "subfolder_list = [\"Old_2019\", \"New_2021\"]\n",
    "\n",
    "base_ids_set = set()\n",
    "full_ids_set = set()\n",
    "\n",
    "for movie in movie_list:\n",
    "    for subfolder in subfolder_list:\n",
    "        trial_path = os.path.join(base_path, movie, subfolder, \"trialdata.csv\")\n",
    "        print(f\"\\nðŸ“ Reading data for: {movie} / {subfolder}\")\n",
    "        if not os.path.exists(trial_path):\n",
    "            continue\n",
    "        df = pd.read_csv(trial_path, header=None, comment=\"#\")\n",
    "        df.columns = [\"ID\", \"Trial\", \"Timestamp\", \"Datastring\"]\n",
    "        print(\"  Participants in trial_data:\", df[\"ID\"].nunique())\n",
    "        df[\"BaseID\"] = df[\"ID\"].astype(str).apply(lambda x: x.split(\":\")[0])\n",
    "        base_ids_set.update(df[\"BaseID\"].unique())\n",
    "        full_ids_set.update(df[\"ID\"].unique())\n",
    "\n",
    "sorted_ids = sorted(base_ids_set)\n",
    "user_id_map = {bid: f\"{i+1:03d}\" for i, bid in enumerate(sorted_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff91767-cf1d-4c7f-8e7f-6a85a779275c",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b242924-3eec-4aad-943e-8a8c0c670648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(base_path, movie, subfolder):\n",
    "    \"\"\"Read trial, event, and question data for a given movie and subfolder.\"\"\"\n",
    "    data_path = os.path.join(base_path, movie, subfolder)\n",
    "\n",
    "    trial_path = os.path.join(data_path, \"trialdata.csv\")\n",
    "    event_path = os.path.join(data_path, \"eventdata.csv\")\n",
    "    question_path = os.path.join(data_path, \"questiondata.csv\")\n",
    "\n",
    "    # Load trial data\n",
    "    trial_data = pd.read_csv(trial_path, header=None, comment=\"#\")\n",
    "    trial_data.columns = [\"ID\", \"Trial\", \"Timestamp\", \"Datastring\"]\n",
    "\n",
    "    # Load event data\n",
    "    event_data = pd.read_csv(event_path, header=None, comment=\"#\")\n",
    "    event_data.columns = [\"ID\", \"Event\", \"Duration\", \"Details\", \"Timestamp\"]\n",
    "\n",
    "    # Load question data\n",
    "    question_data = pd.read_csv(question_path, header=None, comment=\"#\")\n",
    "    question_data.columns = [\"ID\", \"Question\", \"Response\"]\n",
    "\n",
    "    return trial_data, event_data, question_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0aa88-5af6-4e47-ac1f-139edeae4177",
   "metadata": {},
   "source": [
    "### Drop Incomplete Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e958651f-421d-4788-b2ee-a5bcfb36e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_completed_and_audio_catch(trial_data, event_data, question_data):\n",
    "    \"\"\"Keep only participants who appear in question_data and have completed Audio_Catch.\"\"\"\n",
    "    valid_ids = question_data[\"ID\"].unique()\n",
    "\n",
    "    audio_ids = question_data.loc[\n",
    "        question_data[\"Question\"] == \"Audio_Catch\", \"ID\"\n",
    "    ].unique()\n",
    "\n",
    "    keep_ids = set(valid_ids) & set(audio_ids)\n",
    "\n",
    "    trial_data = trial_data[trial_data[\"ID\"].isin(keep_ids)].copy()\n",
    "    event_data = event_data[event_data[\"ID\"].isin(keep_ids)].copy()\n",
    "    question_data = question_data[question_data[\"ID\"].isin(keep_ids)].copy()\n",
    "\n",
    "    return trial_data, event_data, question_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d09b87-d908-413b-8287-0a02f9f5b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_loops(trial_data, event_data, question_data, instr_threshold=2):\n",
    "    \"\"\"Remove participants who looped (repeated) experiments due to technical issue. \n",
    "       Filter by participants who clicked \"begin\" twice\n",
    "    \"\"\"\n",
    "    def cnt(series):\n",
    "        c = 0\n",
    "        for ds in series.dropna():\n",
    "            try:\n",
    "                js = json.loads(ds)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                continue\n",
    "            if str(js.get(\"phase\")).lower() == \"id_submit\" and str(js.get(\"status\")).lower() in {\"begin\", \"submit\"}:\n",
    "                c += 1\n",
    "        return c\n",
    "\n",
    "    counts = trial_data.groupby(\"ID\")[\"Datastring\"].apply(cnt)\n",
    "\n",
    "    drop_ids = set(counts[counts > 2].index.astype(str))\n",
    "    \n",
    "    EXEMPT_IDS = {\"debug8CU6p:debugHfGpS\", \"debugGELZw:debugBfrPh\"} # subject looped but not in middle of experiment phase\n",
    "    \n",
    "    drop_ids -= {str(x) for x in EXEMPT_IDS}\n",
    "\n",
    "    trial_data = trial_data[~trial_data[\"ID\"].isin(drop_ids)].copy()\n",
    "    event_data = event_data[~event_data[\"ID\"].isin(drop_ids)].copy()\n",
    "    question_data = question_data[~question_data[\"ID\"].isin(drop_ids)].copy()\n",
    "\n",
    "    print(f\"looped participants: {drop_ids}\")\n",
    "    \n",
    "    return trial_data, event_data, question_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19f5ce3c-ae11-4aed-9833-9137f8ced738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trials(trial_data, question_data, correct_map):\n",
    "    \"\"\"Filter participants who passed comprehension test: all correct on 4 questions\"\"\"\n",
    "    def phase(q):\n",
    "        if isinstance(q, str):\n",
    "            if \"Comp_\" in q:\n",
    "                return \"comprehension\"\n",
    "            if \"Audio\" in q:\n",
    "                return \"audio_catch\"\n",
    "        return None\n",
    "\n",
    "    question_data[\"phase\"] = question_data[\"Question\"].apply(phase)\n",
    "\n",
    "    comp = question_data[question_data[\"phase\"]==\"comprehension\"].copy()\n",
    "    comp[\"Response\"] = pd.to_numeric(comp[\"Response\"], errors=\"coerce\")\n",
    "    comp[\"acc\"] = (comp[\"Response\"] == comp[\"Question\"].map(correct_map)).astype(int)\n",
    "\n",
    "    audio = question_data[question_data[\"phase\"]==\"audio_catch\"].copy()\n",
    "    audio[\"acc\"] = (audio[\"Response\"] == audio[\"counterbalance\"]).astype(int)\n",
    "\n",
    "    drop_ids = set(audio.loc[audio[\"acc\"]==0, \"ID\"]) | set(comp.loc[comp[\"acc\"]==0, \"ID\"])\n",
    "\n",
    "    return trial_data[~trial_data[\"ID\"].isin(drop_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48272c59-30e0-4a23-9493-516fe3975f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pilots(trial_data, question_data, event_data, THRESHOLD = 1555729200000):\n",
    "    \"\"\"Filter participants who start study after the study time. exclude pilot subjects\"\"\"\n",
    "\n",
    "    trial_data[\"Timestamp\"] = pd.to_numeric(trial_data[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    early_ids = trial_data.loc[trial_data[\"Timestamp\"] < THRESHOLD, \"OriginalID\"] \\\n",
    "                          .dropna().unique().tolist()\n",
    "    drop_pilots = list(set(early_ids))\n",
    "    # print(drop_pilots)\n",
    "\n",
    "    trial_data = trial_data[~trial_data[\"OriginalID\"].isin(drop_pilots)]\n",
    "    question_data = question_data[~question_data[\"ID\"].isin(drop_pilots)]\n",
    "    event_data = event_data[~event_data[\"ID\"].isin(drop_pilots)]\n",
    "\n",
    "    return trial_data, question_data, event_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc743707-fc95-4581-a3c2-3b9548cfee92",
   "metadata": {},
   "source": [
    "### Add New Unique ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb49025-0ad4-4f76-a007-de2b1abec03d",
   "metadata": {},
   "source": [
    "Anonymized participant IDs were generated using a standardized format that includes task type, year, and movie number (e.g., \"P2023M12_001\"). \r\n",
    "\r\n",
    "Base IDs were extracted from the original ID column, mapped to unique numbers, and combined with the corresponding prefix. \r\n",
    "\r\n",
    "The original ID values were preserved in a new column (`OriginalID`) for traceability. This process was applied consistently to all datasts.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d681001-6568-4c54-8009-cbf00d12ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_new_user_ids(trial_data, event_data, question_data, user_id_map, task, year, movie):\n",
    "\n",
    "    def map_ids(df):\n",
    "        df = df.copy()\n",
    "        df[\"BaseID\"] = df[\"ID\"].astype(str).apply(lambda x: x.split(\":\")[0])\n",
    "\n",
    "        df[\"OriginalID\"] = df[\"ID\"] \n",
    "\n",
    "        df[\"UniqueNum\"] = df[\"BaseID\"].map(user_id_map)\n",
    "\n",
    "        prefix = f\"{task}{year}M{int(movie):02d}_\"\n",
    "\n",
    "        df[\"ID\"] = df[\"UniqueNum\"].apply(lambda x: f\"{prefix}{str(x).zfill(3)}\" if pd.notnull(x) else None)\n",
    "\n",
    "        return df.drop(columns=[\"BaseID\", \"UniqueNum\"])\n",
    "\n",
    "    trial_data = map_ids(trial_data)\n",
    "    event_data = map_ids(event_data)\n",
    "    question_data = map_ids(question_data)\n",
    "\n",
    "    return trial_data, event_data, question_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ff579-783c-40c9-9926-6d36fd174a72",
   "metadata": {},
   "source": [
    "### Extract Variables\n",
    "Counterbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a1ec6ba-f535-4d1f-9e41-f041e97488ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_counterbalance(trial_df, event_df, question_df):\n",
    "    label_temp = trial_df[trial_df[\"Datastring\"].str.contains(\"prac_segA_pred1\", na=False)].copy()\n",
    "    label_temp[\"BaseID\"] = label_temp[\"ID\"].astype(str).str.split(\":\").str[0]\n",
    "    label_temp[\"counterbalance\"] = label_temp[\"Datastring\"].str.extract(r'\"counterbalance\"\\s*:\\s*([0-5])')[0]\n",
    "    cb_map = dict(zip(label_temp[\"BaseID\"], label_temp[\"counterbalance\"]))\n",
    "\n",
    "    def apply_map(df):\n",
    "        df = df.copy()\n",
    "        base = df[\"ID\"].astype(str).str.split(\":\").str[0]\n",
    "        df[\"counterbalance\"] = base.map(cb_map)\n",
    "        return df\n",
    "\n",
    "    return apply_map(trial_df), apply_map(event_df), apply_map(question_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a91b226e-ff44-4b47-a7fa-10808f27e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_extract_tag(df, subfolder):\n",
    "    df = df[df[\"Datastring\"].str.contains(r'\"phase\"\\s*:\\s*\"(?:prediction)\"', na=False, regex=True)].copy()\n",
    "\n",
    "    df[\"offset\"] = df[\"Datastring\"].str.extract(r'\"offset\"\\s*:\\s*\"?(.*?)\"?[,}]')\n",
    "    df[\"tag\"] = df[\"Datastring\"].str.extract(r'\"tag\"\\s*:\\s*\"?(.*?)\"?[,}]')\n",
    "    df[\"video_segment\"] = df[\"tag\"].str.extract(r'_seg([A-Za-z0-9]+)_')\n",
    "    df[\"phase_type\"] = df[\"tag\"].apply(\n",
    "        lambda x: \"practice\" if isinstance(x, str) and x.startswith(\"prac_\")\n",
    "        else (\"test\" if isinstance(x, str) and x.startswith(\"test_\") else None)\n",
    "    )\n",
    "\n",
    "    df[\"data_set\"] = subfolder\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_vidpath_prediction_time(df):\n",
    "    \"\"\"Extract vidpath, prediction number, time, and confidence from Datastring.\"\"\"\n",
    "    # vidpath\n",
    "    df[\"vidpath\"] = df[\"Datastring\"].str.extract(r'\"vidpath\"\\s*:\\s*\"([^\"]+)\"')\n",
    "\n",
    "    # prediction number\n",
    "    df[\"prediction_number\"] = df[\"Datastring\"].str.extract(r'\"prediction\"\\s*:\\s*\"?(.*?)\"?(?:,|})')[0]\n",
    "\n",
    "    # time\n",
    "    df[\"time\"] = df[\"Datastring\"].str.extract(r'\"time\"\\s*:\\s*([0-9]+)')\n",
    "\n",
    "    # NEW: confidence\n",
    "    df[\"confidence\"] = df[\"Datastring\"].str.extract(r'\"confidence\"\\s*:\\s*\"?(.*?)\"?(?:,|})')[0]\n",
    "\n",
    "    for col in [\"prediction_number\", \"time\", \"confidence\", \"offset\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_prediction_content(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df[\"prediction_content\"] = df[\"Datastring\"].str.extract(r'\"content\"\\s*:\\s*\"((?:\\\\.|[^\"\\\\])*)\"')\n",
    "\n",
    "    df[\"prediction_content\"] = df[\"prediction_content\"].str.replace(r'\\\\\"', r'\\\\', regex=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aa8b992-39c8-4ce5-a4ed-ee9eac67bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_and_remove_columns(df):\n",
    "    desired_columns = [\n",
    "        'ID', 'OriginalID', 'Trial', 'Timestamp', 'counterbalance', 'phase_type', 'confidence',\n",
    "        'time', 'prediction_number', 'vidpath', 'prediction_content',\n",
    "        'tag', 'offset', 'video_segment', 'data_set'\n",
    "    ]\n",
    "    \n",
    "    # Only keep columns that exist in df\n",
    "    cols_to_keep = [col for col in desired_columns if col in df.columns]\n",
    "    return df[cols_to_keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f605ba-89be-4fd6-b167-e44cb8998d86",
   "metadata": {},
   "source": [
    "## Run Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc26021-6787-4cf4-9f83-060b241557e7",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "939cefc5-0313-4196-83c4-2c8ff4c68387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_movie(movie):\n",
    "    if movie == \"busstop\":\n",
    "        return \"01\"\n",
    "    elif movie == \"cmiyc_long\":\n",
    "        return \"02\"\n",
    "    elif movie == \"keithreynolds\":\n",
    "        return \"03\"\n",
    "    elif movie == \"theboyfriend\":\n",
    "        return \"04\"\n",
    "    elif movie == \"therock\":\n",
    "        return \"05\"\n",
    "    elif movie == \"theshoe\":\n",
    "        return \"06\"\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def map_year(year):\n",
    "    if year == \"Old_2019\":\n",
    "        return \"2019\"\n",
    "    elif year == \"New_2021\":\n",
    "        return \"2021\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84c352bc-c753-4a9d-9a61-b1183447ab8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "busstop\n",
      "looped participants: set()\n",
      "Old_2019: data entries: 1753\n",
      "looped participants: {'debugs2Uin:debugUss8N', 'debug617Hp:debugox7Nx', 'debugsgaz9:debugtsrSz', 'debugcWTZl:debugetWm9', 'debugE6AVE:debugdFR5k', 'debugBxi4k:debugiNh31', 'debugoO7XR:debugdSU0R'}\n",
      "New_2021: data entries: 679\n",
      "total number of rows:2432\n",
      "total number of participants: 198\n",
      "\n",
      "cmiyc_long\n",
      "looped participants: set()\n",
      "Old_2019: data entries: 2112\n",
      "looped participants: {'debugNSQx8:debugypdvf', 'debugm3tNI:debug9KzKf'}\n",
      "New_2021: data entries: 663\n",
      "total number of rows:2775\n",
      "total number of participants: 181\n",
      "\n",
      "keithreynolds\n",
      "looped participants: set()\n",
      "Old_2019: data entries: 1289\n",
      "looped participants: {'debugTD2AD:debugS2JLT', 'debugRl5wZ:debugBEuIb'}\n",
      "New_2021: data entries: 438\n",
      "total number of rows:1727\n",
      "total number of participants: 184\n",
      "\n",
      "theboyfriend\n",
      "looped participants: set()\n",
      "Old_2019: data entries: 1665\n",
      "looped participants: set()\n",
      "New_2021: data entries: 251\n",
      "total number of rows:1916\n",
      "total number of participants: 169\n",
      "\n",
      "therock\n",
      "looped participants: set()\n",
      "Old_2019: data entries: 1139\n",
      "looped participants: set()\n",
      "New_2021: data entries: 460\n",
      "total number of rows:1599\n",
      "total number of participants: 182\n",
      "\n",
      "theshoe\n",
      "looped participants: set()\n",
      "Old_2019: data entries: 960\n",
      "looped participants: {'debugokHY2:debug7O47g'}\n",
      "New_2021: data entries: 117\n",
      "total number of rows:1077\n",
      "total number of participants: 181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../../Raw Data/Prediction Raw Psiturk Data Files\"\n",
    "movie_list = [\"busstop\", \"cmiyc_long\", \"keithreynolds\", \"theboyfriend\", \"therock\", \"theshoe\"]\n",
    "subfolder_list = [\"Old_2019\", \"New_2021\"]\n",
    "\n",
    "datasets_dic = {}\n",
    "instr_threshold = 9\n",
    "correct_map = {\"Comp_Q1\":1, \"Comp_Q2\":3, \"Comp_Q3\":4, \"Comp_Q4\":1}\n",
    "THRESHOLD = 1555729200000\n",
    "\n",
    "\n",
    "for movie in movie_list:\n",
    "    print(movie)\n",
    "    if movie == \"theshoe\":\n",
    "        correct_map = {\"Comp_Q1\":2, \"Comp_Q2\":4, \"Comp_Q3\":1, \"Comp_Q4\":3}\n",
    "        THRESHOLD = 1555027140000 # April 11, 2019\n",
    "    else:\n",
    "        correct_map = {\"Comp_Q1\":1, \"Comp_Q2\":3, \"Comp_Q3\":4, \"Comp_Q4\":1}\n",
    "        THRESHOLD = 1555729200000 # April 19, 2019\n",
    "\n",
    "    datasets = []\n",
    "    for subfolder in subfolder_list:\n",
    "        trial_data, event_data, question_data = read_data(base_path, movie, subfolder)\n",
    "        \n",
    "        trial_data, event_data, question_data = filter_completed_and_audio_catch(trial_data, event_data, question_data)\n",
    "        trial_data, event_data, question_data = filter_loops(trial_data, event_data, question_data) \n",
    "        \n",
    "        trial_data, event_data, question_data = add_counterbalance(trial_data, event_data, question_data)\n",
    "        \n",
    "        trial_data = filter_trials(trial_data, question_data, correct_map = correct_map)\n",
    "        \n",
    "        a = map_year(subfolder)\n",
    "        b = map_movie(movie)\n",
    "        trial_data, even_data, question_data = assign_new_user_ids(trial_data, event_data, question_data, user_id_map, \n",
    "                                                                   \"P\", a, b)\n",
    "        trial_data, event_data, question_data = filter_pilots(trial_data, question_data, event_data, THRESHOLD)\n",
    "        \n",
    "        trial_data = filter_and_extract_tag(trial_data, subfolder)\n",
    "        trial_data = extract_vidpath_prediction_time(trial_data)\n",
    "        trial_data = extract_prediction_content(trial_data)\n",
    "        trial_data = reorder_and_remove_columns(trial_data)\n",
    "\n",
    "        print(subfolder+\": data entries: \"+str(len(trial_data)))\n",
    "        \n",
    "        datasets.append(trial_data)\n",
    "\n",
    "    df_all = pd.concat(datasets, ignore_index=True)\n",
    "    print(f\"total number of rows:\"+ str(len(df_all)))\n",
    "    print(\"total number of participants: \" + str(df_all[\"ID\"].nunique()) + \"\\n\")\n",
    "    datasets_dic[movie] = df_all\n",
    "    \n",
    "    df_all.drop(\"OriginalID\", axis = 1)\n",
    "\n",
    "    filename = f\"{movie}_prediction_cleaned.csv\"\n",
    "    output_dir = \"../../Cleaned Data/Prediction_Cleaned_Data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    df_all.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df193f2-3fea-4e22-9c84-8b4a9d9c925b",
   "metadata": {},
   "source": [
    "Data in each Movie:\n",
    "`busstop`: 1753 + 679 = 2432\n",
    "`cmiyc_long`: 2112 + 663 = 2775\n",
    "`keithreynolds`: 1289 + 438 = 1727\n",
    "`theboyfriend`: 1665 + 251 = 1916\n",
    "`therock`: 1139 + 460 = 1599\n",
    "`theshoe`: 960 + 117 = 1077\n",
    "\n",
    "Removed subjects that looped: \n",
    "`busstop`: {'debugsgaz9:debugtsrSz', 'debugcWTZl:debugetWm9', 'debugBxi4k:debugiNh31', 'debugs2Uin:debugUss8N', 'debug617Hp:debugox7Nx', 'debugoO7XR:debugdSU0R', 'debugE6AVE:debugdFR5k'}\n",
    "`cmiyc_long`: {'debugm3tNI:debug9KzKf', 'debugNSQx8:debugypdvf'}\n",
    "`keithreynolds`: {'debugRl5wZ:debugBEuIb', 'debugTD2AD:debugS2JLT'}\n",
    "`theshoe`: {'debugokHY2:debug7O47g'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a4da6d-7086-4695-9551-117a97ecd689",
   "metadata": {},
   "source": [
    "## Save ID map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed60df3f-53bd-4a4c-86c8-f5001c95afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat(datasets_dic, names=[\"Movie\"], ignore_index=False).reset_index(level=0).reset_index(drop=True)\n",
    "\n",
    "user_id_map_df = combined[[\"ID\", \"OriginalID\",\"Movie\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "user_id_map_df.to_csv(\"../../Cleaned Data/Prediction_Cleaned_Data/prediction_user_id_map.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
